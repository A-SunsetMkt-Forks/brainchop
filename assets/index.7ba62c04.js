import{t as ae,s as We,d as G,l as ke,a as be,b as ee,c as xe,e as Se,E as ie,m as Re,o as ce,z as ue,f as L,g as qe,w as _e,h as ze,i as Me,j as H,k as ve,n as Ge,p as je,q as Te,r as Pe,u as de,v as Qe,x as oe,y as He,N as Xe}from"./vendor.5623a205.js";const Ue=function(){const e=document.createElement("link").relList;if(e&&e.supports&&e.supports("modulepreload"))return;for(const p of document.querySelectorAll('link[rel="modulepreload"]'))c(p);new MutationObserver(p=>{for(const i of p)if(i.type==="childList")for(const u of i.addedNodes)u.tagName==="LINK"&&u.rel==="modulepreload"&&c(u)}).observe(document,{childList:!0,subtree:!0});function n(p){const i={};return p.integrity&&(i.integrity=p.integrity),p.referrerpolicy&&(i.referrerPolicy=p.referrerpolicy),p.crossorigin==="use-credentials"?i.credentials="include":p.crossorigin==="anonymous"?i.credentials="omit":i.credentials="same-origin",i}function c(p){if(p.ep)return;p.ep=!0;const i=n(p);fetch(p.href,i)}};Ue();const Ze={batchSize:1,numOfChan:1,isColorEnable:!0,isAutoColors:!0,bgLabelValue:0,drawBoundingVolume:!1,isGPU:!0,isBrainCropMaskBased:!0,showPhase1Output:!1,isPostProcessEnable:!0,isContoursViewEnable:!1,browserArrayBufferMaxZDim:30,telemetryFlag:!1,chartXaxisStepPercent:10,uiSampleName:"BC_UI_Sample",atlasSelectedColorTable:"Fire"},D=[{id:1,type:"Segmentation",path:"/models/model5_gw_ae/model.json",modelName:"\u26A1 Tissue GWM (light)",colormapPath:"./models/model5_gw_ae/colormap3.json",preModelId:null,preModelPostProcess:!1,isBatchOverlapEnable:!1,numOverlapBatches:0,enableTranspose:!0,enableCrop:!0,cropPadding:18,autoThreshold:0,enableQuantileNorm:!1,filterOutWithPreMask:!1,enableSeqConv:!1,textureSize:0,warning:null,inferenceDelay:100,description:"Gray and white matter segmentation model. Operates on full T1 image in a single pass, but uses only 5 filters per layer. Can work on integrated graphics cards but is barely large enough to provide good accuracy. Still more accurate than the subvolume model."},{id:2,type:"Segmentation",path:"/models/model20chan3cls/model.json",modelName:"\u{1F52A} Tissue GWM (High Acc)",colormapPath:"./models/model20chan3cls/colormap.json",preModelId:null,preModelPostProcess:!1,isBatchOverlapEnable:!1,numOverlapBatches:0,enableTranspose:!0,enableCrop:!0,cropPadding:0,autoThreshold:.2,enableQuantileNorm:!0,filterOutWithPreMask:!1,enableSeqConv:!1,textureSize:0,warning:"This model may need dedicated graphics card.  For more info please check with Browser Resources <i class='fa fa-cogs'></i>.",inferenceDelay:100,description:"Gray and white matter segmentation model. Operates on full T1 image in a single pass but needs a dedicated graphics card to operate. Provides the best accuracy with hard cropping for better speed"},{id:3,type:"Segmentation",path:"/models/model20chan3cls/model.json",modelName:"\u{1F52A} Tissue GWM (High Acc, Low Mem)",colormapPath:"./models/model20chan3cls/colormap.json",preModelId:null,preModelPostProcess:!1,isBatchOverlapEnable:!1,numOverlapBatches:0,enableTranspose:!0,enableCrop:!0,cropPadding:0,autoThreshold:.2,enableQuantileNorm:!0,filterOutWithPreMask:!1,enableSeqConv:!0,textureSize:0,warning:"This model may need dedicated graphics card.  For more info please check with Browser Resources <i class='fa fa-cogs'></i>.",inferenceDelay:100,description:"Gray and white matter segmentation model. Operates on full T1 image in a single pass but needs a dedicated graphics card to operate. Provides high accuracy and fit low memory available but slower"},{id:4,type:"Atlas",path:"/models/model30chan18cls/model.json",modelName:"\u{1FA93} Subcortical + GWM (High Mem, Fast)",colormapPath:"./models/model30chan18cls/colormap.json",preModelId:null,preModelPostProcess:!1,isBatchOverlapEnable:!1,numOverlapBatches:200,enableTranspose:!0,enableCrop:!0,cropPadding:0,autoThreshold:.2,enableQuantileNorm:!1,filterOutWithPreMask:!1,enableSeqConv:!1,textureSize:0,warning:"This model may need dedicated graphics card.  For more info please check with Browser Resources <i class='fa fa-cogs'></i>.",inferenceDelay:100,description:"Parcellation of the brain into 17 regions: gray and white matter plus subcortical areas. This is a robust model able to handle range of data quality, including varying saturation, and even clinical scans. It may work on infant brains, but your mileage may vary."},{id:5,type:"Atlas",path:"/models/model30chan18cls/model.json",modelName:"\u{1FA93} Subcortical + GWM (Low Mem, Slow)",colormapPath:"./models/model30chan18cls/colormap.json",preModelId:null,preModelPostProcess:!1,isBatchOverlapEnable:!1,numOverlapBatches:200,enableTranspose:!0,enableCrop:!0,cropPadding:0,autoThreshold:.2,enableQuantileNorm:!1,filterOutWithPreMask:!1,enableSeqConv:!0,textureSize:0,warning:"This model may need dedicated graphics card.  For more info please check with Browser Resources <i class='fa fa-cogs'></i>.",inferenceDelay:100,description:"Parcellation of the brain into 17 regions: gray and white matter plus subcortical areas. This is a robust model able to handle range of data quality, including varying saturation, and even clinical scans. It may work on infant brains, but your mileage may vary."},{id:6,type:"Atlas",path:"/models/model18cls/model.json",modelName:"\u{1FA93} Subcortical + GWM (Low Mem, Faster)",colormapPath:"./models/model18cls/colormap.json",preModelId:null,preModelPostProcess:!1,isBatchOverlapEnable:!1,numOverlapBatches:200,enableTranspose:!0,enableCrop:!0,cropPadding:0,autoThreshold:.2,enableQuantileNorm:!1,filterOutWithPreMask:!1,enableSeqConv:!0,textureSize:0,warning:"This model may need dedicated graphics card.  For more info please check with Browser Resources <i class='fa fa-cogs'></i>.",inferenceDelay:100,description:"Parcellation of the brain into 17 regions: gray and white matter plus subcortical areas. This is a robust model able to handle range of data quality, including varying saturation, and even clinical scans. It may work on infant brains, but your mileage may vary."},{id:7,type:"Atlas",path:"/models/model30chan18cls/model.json",modelName:"\u{1F52A}\u{1FA93} Subcortical + GWM (Failsafe, Less Acc)",colormapPath:"./models/model30chan18cls/colormap.json",preModelId:1,preModelPostProcess:!1,isBatchOverlapEnable:!1,numOverlapBatches:200,enableTranspose:!0,enableCrop:!0,cropPadding:0,autoThreshold:0,enableQuantileNorm:!1,filterOutWithPreMask:!1,enableSeqConv:!1,textureSize:0,warning:"This model may need dedicated graphics card.  For more info please check with Browser Resources <i class='fa fa-cogs'></i>.",inferenceDelay:100,description:"Parcellation of the brain into 17 regions: gray and white matter plus subcortical areas. This is not a robust model, it may work on low data quality, including varying saturation, and even clinical scans. It may work also on infant brains, but your mileage may vary."},{id:8,type:"Atlas",path:"/models/model30chan50cls/model.json",modelName:"\u{1F52A} Aparc+Aseg 50 (High Mem, Fast)",colormapPath:"./models/model30chan50cls/colormap.json",preModelId:1,preModelPostProcess:!1,isBatchOverlapEnable:!1,numOverlapBatches:200,enableTranspose:!0,enableCrop:!0,cropPadding:0,autoThreshold:0,enableQuantileNorm:!0,filterOutWithPreMask:!1,enableSeqConv:!1,textureSize:0,warning:"This model may need dedicated graphics card.  For more info please check with Browser Resources <i class='fa fa-cogs'></i>.",inferenceDelay:100,description:"This is a 50-class model, that segments the brain into the Aparc+Aseg Freesurfer Atlas but one where cortical homologues are merged into a single class."},{id:9,type:"Atlas",path:"/models/model30chan50cls/model.json",modelName:"\u{1F52A} Aparc+Aseg 50 (Low Mem, Slow)",colormapPath:"./models/model30chan50cls/colormap.json",preModelId:1,preModelPostProcess:!1,isBatchOverlapEnable:!1,numOverlapBatches:200,enableTranspose:!0,enableCrop:!0,cropPadding:0,autoThreshold:0,enableQuantileNorm:!0,filterOutWithPreMask:!1,enableSeqConv:!0,textureSize:0,warning:"This model may need dedicated graphics card.  For more info please check with Browser Resources <i class='fa fa-cogs'></i>.",inferenceDelay:100,description:"This is a 50-class model, that segments the brain into the Aparc+Aseg Freesurfer Atlas but one where cortical homologues are merged into a single class. The model use sequential convolution for inference to overcome browser memory limitations but leads to longer computation time."},{id:10,type:"Brain_Extraction",path:"/models/model5_gw_ae/model.json",modelName:"\u26A1 Extract the Brain (FAST)",preModelId:null,preModelPostProcess:!1,isBatchOverlapEnable:!1,numOverlapBatches:0,enableTranspose:!0,enableCrop:!0,cropPadding:18,autoThreshold:0,enableQuantileNorm:!1,filterOutWithPreMask:!1,enableSeqConv:!1,textureSize:0,warning:null,inferenceDelay:100,description:"Extract the brain fast model operates on full T1 image in a single pass, but uses only 5 filters per layer. Can work on integrated graphics cards but is barely large enough to provide good accuracy. Still more accurate than the failsafe version."},{id:11,type:"Brain_Extraction",path:"/models/model11_gw_ae/model.json",modelName:"\u{1F52A} Extract the Brain (High Acc, Slow)",preModelId:null,preModelPostProcess:!1,isBatchOverlapEnable:!1,numOverlapBatches:0,enableTranspose:!0,enableCrop:!0,cropPadding:0,autoThreshold:0,enableQuantileNorm:!1,filterOutWithPreMask:!1,enableSeqConv:!0,textureSize:0,warning:"This model may need dedicated graphics card.  For more info please check with Browser Resources <i class='fa fa-cogs'></i>.",inferenceDelay:100,description:"Extract the brain high accuracy model operates on full T1 image in a single pass, but uses only 11 filters per layer. Can work on dedicated graphics cards. Still more accurate than the fast version."},{id:12,type:"Brain_Masking",path:"/models/model5_gw_ae/model.json",modelName:"\u26A1 Brain Mask (FAST)",colormapPath:"./models/model5_gw_ae/colormap.json",preModelId:null,preModelPostProcess:!1,isBatchOverlapEnable:!1,numOverlapBatches:0,enableTranspose:!0,enableCrop:!0,cropPadding:17,autoThreshold:0,enableQuantileNorm:!1,filterOutWithPreMask:!1,enableSeqConv:!1,textureSize:0,warning:null,inferenceDelay:100,description:"This fast masking model operates on full T1 image in a single pass, but uses only 5 filters per layer. Can work on integrated graphics cards but is barely large enough to provide good accuracy. Still more accurate than failsafe version."},{id:13,type:"Brain_Masking",path:"/models/model11_gw_ae/model.json",modelName:"\u{1F52A} Brain Mask (High Acc, Low Mem)",preModelId:null,preModelPostProcess:!1,isBatchOverlapEnable:!1,numOverlapBatches:0,enableTranspose:!0,enableCrop:!0,cropPadding:0,autoThreshold:0,enableQuantileNorm:!0,filterOutWithPreMask:!1,enableSeqConv:!0,textureSize:0,warning:"This model may need dedicated graphics card.  For more info please check with Browser Resources <i class='fa fa-cogs'></i>.",inferenceDelay:100,description:"This masking model operates on full T1 image in a single pass, but uses 11 filters per layer. Can work on dedicated graphics cards. Still more accurate than fast version."},{id:14,type:"Atlas",path:"/models/model21_104class/model.json",modelName:"\u{1F52A} Aparc+Aseg 104 (High Mem, Fast)",colormapPath:"./models/model21_104class/colormap.json",preModelId:0,preModelPostProcess:!1,isBatchOverlapEnable:!1,numOverlapBatches:200,enableTranspose:!0,enableCrop:!0,cropPadding:0,autoThreshold:0,enableQuantileNorm:!1,filterOutWithPreMask:!1,enableSeqConv:!1,textureSize:0,warning:"This model may need dedicated graphics card.  For more info please check with Browser Resources <i class='fa fa-cogs'></i>.",inferenceDelay:100,description:"FreeSurfer aparc+aseg atlas 104 parcellate brain areas into 104 regions. It contains a combination of the Desikan-Killiany atlas for cortical area and also segmentation of subcortical regions."},{id:15,type:"Atlas",path:"/models/model21_104class/model.json",modelName:"\u{1F52A} Aparc+Aseg 104 (Low Mem, Slow)",colormapPath:"./models/model21_104class/colormap.json",preModelId:0,preModelPostProcess:!1,isBatchOverlapEnable:!1,numOverlapBatches:200,enableTranspose:!0,enableCrop:!0,cropPadding:0,autoThreshold:0,enableQuantileNorm:!1,filterOutWithPreMask:!1,enableSeqConv:!0,textureSize:0,warning:"This model may need dedicated graphics card.  For more info please check with Browser Resources <i class='fa fa-cogs'></i>.",inferenceDelay:100,description:"FreeSurfer aparc+aseg atlas 104 parcellate brain areas into 104 regions. It contains a combination of the Desikan-Killiany atlas for cortical area and also segmentation of subcortical regions. The model use sequential convolution for inference to overcome browser memory limitations but leads to longer computation time. "}];class Ke{idx(e,n,c,p){return c*p[0]*p[1]+n*p[0]+e}check_previous_slice(e,n,c,p,i,u,a,f,d,o){let s=0;if(!i)return 0;const l=e[this.idx(c,p,i,u)];if(a>=6){const t=this.idx(c,p,i-1,u);l===e[t]&&(d[s++]=n[t])}if(a>=18){if(c){const t=this.idx(c-1,p,i-1,u);l===e[t]&&(d[s++]=n[t])}if(p){const t=this.idx(c,p-1,i-1,u);l===e[t]&&(d[s++]=n[t])}if(c<u[0]-1){const t=this.idx(c+1,p,i-1,u);l===e[t]&&(d[s++]=n[t])}if(p<u[1]-1){const t=this.idx(c,p+1,i-1,u);l===e[t]&&(d[s++]=n[t])}}if(a===26){if(c&&p){const t=this.idx(c-1,p-1,i-1,u);l===e[t]&&(d[s++]=n[t])}if(c<u[0]-1&&p){const t=this.idx(c+1,p-1,i-1,u);l===e[t]&&(d[s++]=n[t])}if(c&&p<u[1]-1){const t=this.idx(c-1,p+1,i-1,u);l===e[t]&&(d[s++]=n[t])}if(c<u[0]-1&&p<u[1]-1){const t=this.idx(c+1,p+1,i-1,u);l===e[t]&&(d[s++]=n[t])}}return s?(this.fill_tratab(f,d,s,o),d[0]):0}do_initial_labelling(e,n,c){const p=new Uint32Array(32),i=new Uint32Array(32);let u=1;const a=8192;let f=a,d=new Uint32Array(f).fill(0);const o=new Uint32Array(n[0]*n[1]*n[2]).fill(0),s=new Uint32Array(27);for(let l=0;l<n[2];l++)for(let t=0;t<n[1];t++)for(let h=0;h<n[0];h++){let g=0;const y=e[this.idx(h,t,l,n)];if(y!==0){if(s[0]=this.check_previous_slice(e,o,h,t,l,n,c,d,p,i),s[0]&&(g+=1),c>=6){if(h){const m=this.idx(h-1,t,l,n);y===e[m]&&(s[g++]=o[m])}if(t){const m=this.idx(h,t-1,l,n);y===e[m]&&(s[g++]=o[m])}}if(c>=18){if(t&&h){const m=this.idx(h-1,t-1,l,n);y===e[m]&&(s[g++]=o[m])}if(t&&h<n[0]-1){const m=this.idx(h+1,t-1,l,n);y===e[m]&&(s[g++]=o[m])}}if(g)o[this.idx(h,t,l,n)]=s[0],this.fill_tratab(d,s,g,i);else{if(o[this.idx(h,t,l,n)]=u,u>=f){f+=a;const m=new Uint32Array(f);m.set(d),d=m}d[u-1]=u,u++}}}for(let l=0;l<u-1;l++){let t=l;for(;d[t]!==t+1;)t=d[t]-1;d[l]=t+1}return[u-1,d,o]}fill_tratab(e,n,c,p){let u=2147483647;for(let a=0;a<c;a++){let f=n[a];for(;e[f-1]!==f;)f=e[f-1];p[a]=f,u=Math.min(u,f)}for(let a=0;a<c;a++)e[p[a]-1]=u}translate_labels(e,n,c,p){const i=n[0]*n[1]*n[2];let u=0;const a=new Uint32Array(i).fill(0);for(let o=0;o<p;o++)u=Math.max(u,c[o]);const f=new Uint32Array(u).fill(0);let d=0;for(let o=0;o<i;o++)e[o]&&(f[c[e[o]-1]-1]||(d+=1,f[c[e[o]-1]-1]=d),a[o]=f[c[e[o]-1]-1]);return[d,a]}largest_original_cluster_labels(e,n,c){const p=e.length,i=new Uint32Array(n+1).fill(0),u=new Uint32Array(n+1).fill(0);for(let d=0;d<p;d++){const o=e[d],s=c[d];i[s]=o,u[s]++}let a=0;for(let d=0;d<n+1;d++){const o=i[d];a=Math.max(a,o);for(let s=0;s<n+1;s++)s!==d&&o===i[s]&&(u[d]<u[s]||u[d]===u[s]&&d<s)&&(i[d]=0)}const f=new Uint32Array(p).fill(0);for(let d=0;d<p;d++)f[d]=i[c[d]];return[a,f]}bwlabel(e,n,c=26,p=!1,i=!1){const u=Date.now(),a=n[0]*n[1]*n[2],f=new Uint32Array(a).fill(0);if(![6,18,26].includes(c))return console.log("bwlabel: conn must be 6, 18 or 26."),[0,f];if(n[0]<2||n[1]<2||n[2]<1)return console.log("bwlabel: img must be 2 or 3-dimensional"),[0,f];if(p)for(let h=0;h<a;h++)e[h]!==0&&(f[h]=1);else f.set(e);let[d,o,s]=this.do_initial_labelling(f,n,c);o===void 0&&(o=new Uint32Array(0));const[l,t]=this.translate_labels(s,n,o,d);if(console.log(c+" neighbor clustering into "+l+" regions in "+(Date.now()-u)+"ms"),i){const[h,g]=this.largest_original_cluster_labels(f,l,t);return[h,g]}return[l,t]}}async function Ce(r,e=[1,1],n=[1,1],c=[1,1]){if(r.rank!==3)throw new Error("Tensor must be 3D");return r.pad([e,n,c])}async function Le(r,e){const n=r.max(),c=n.mul(e),p=await c.data();return n.dispose(),c.dispose(),ee(()=>r.clone().greater(p[0]))}async function pe(r){const e=0;return r.step(e)}async function Je(r,e=.01,n=.99){const c=r.flatten(),p=await c.array();p.sort((t,h)=>t-h);const i=xe(p),u=i.shape[0],a=Math.floor(u*e),f=Math.ceil(u*n)-1,d=i.slice(a,1),o=i.slice(f,1),s=(await d.array())[0],l=(await o.array())[0];return c.dispose(),i.dispose(),d.dispose(),o.dispose(),{qmin:s,qmax:l}}async function $e(r,e,n,c,p,i,u){const a=r.shape[4],f=e.shape[4];let d=null;for(let o=0;o<f;o++){const s=Math.ceil(a/u),l=n.slice([o],[1]);let t=null;for(let g=0;g<s;g++){const y=g*u,m=Math.min((g+1)*u,a);if(y<a){const P=ee(()=>{const M=r.slice([0,0,0,0,y],[-1,-1,-1,-1,m-y]),N=e.slice([0,0,0,y,o],[-1,-1,-1,m-y,1]);return Se(M,N,c,p,"NDHWC",i)});if(t===null)t=P;else{const M=t.add(P);t.dispose(),P.dispose(),t=M}}}const h=t.add(l);if(t.dispose(),l.dispose(),d==null)d=h;else{const g=await Ge([d,h],4);h.dispose(),d.dispose(),d=g}}return d}async function Oe(r,e,n,c){const p=[];for(let d=0;d<r.length;d++)p[d]=Array.from(r[d].dataSync());const i=new Array(p[0].length*p.length);let u=0;for(let d=0;d<p.length;d++)for(let o=0;o<p[d].length;o++)i[u++]=p[d][o];console.log("Done with allOutputSlices3DCC1DimArray ");const a=await pe(xe(i)),f=Array.from(a.dataSync());c(f,e,n)}async function fe(r,e=0){let n=[];e===0?n=await r.max(2).max(1).arraySync():e===1?n=await r.max(2).max(0).arraySync():n=await r.max(1).max(0).arraySync();let c=n.length,p=0;for(let i=0;i<n.length;i++)if(n[i]>0){c=i;break}for(let i=n.length-1;i>=0;i--)if(n[i]>0){p=i;break}return[c,p]}async function Ae(r){const[e,n]=await fe(r,0),[c,p]=await fe(r,1),[i,u]=await fe(r,2);return console.log("row min and max  :",e,n),console.log("col min and max  :",c,p),console.log("depth min and max  :",i,u),[e,n,c,p,i,u]}async function Ye(r,e,n,c,p,i,u,a,f=!0){r[0].dtype!=="int32"&&u("",-1,"generateBrainMask assumes int32"),p.preModelPostProcess&&u("",-1,"generateBrainMask assumes BWLabeler instead of preModelPostProcess");const d=r.length,o=r[0].size,s=d*o,l=new Int32Array(s);let t=0;for(let h=0;h<d;h++)l.set(r[h].dataSync(),t),t+=o;for(let h=0;h<s;h++)l[h]=l[h]!==0?1:0;return(f||i.showPhase1Output)&&(a(l,i,p),u("Segmentation finished",0)),ae(l,[e,n,c])}async function Ie(r,e,n,c,p,i,u,a,f,d){if(f.isPostProcessEnable){const s=new Ke,l=new Uint32Array(e),t=26,h=!0,g=!0,[y,m]=s.bwlabel(r,l,t,h,g);for(let P=0;P<r.length;P++)r[P]*=m[P]}const o=new Uint8Array(r);switch(a.type){case"Brain_Masking":{const s=new Uint8Array(o.length);for(let l=0;l<o.length;l++)s[l]=o[l]!==0?1:0;return s}case"Brain_Extraction":{const s=new Uint8Array(o.length);for(let l=0;l<o.length;l++){const t=o[l]!==0?1:0;s[l]=d[l]*t}return s}}return r}async function De(r,e,n){const c=e.dims[1],p=e.dims[2];let i;if(e.datatypeCode===2)i=new Uint8Array(n);else if(e.datatypeCode===4)i=new Int16Array(n);else if(e.datatypeCode===8)i=new Int32Array(n);else if(e.datatypeCode===16)i=new Float32Array(n);else if(e.datatypeCode===64)i=new Float64Array(n);else if(e.datatypeCode===256)i=new Int8Array(n);else if(e.datatypeCode===512)i=new Uint16Array(n);else if(e.datatypeCode===768)i=new Uint32Array(n);else return;const u=[];let a=0;for(let d=0;d<r;d++){const o=new Array(p*c);let s=0;for(let l=0;l<p;l++)for(let t=0;t<c;t++){const h=i[a++];o[s++]=h&255}u.push(ae(o,[p,c]))}const f=We(u);return G(u),f}async function he(r){return r.layers.length}async function me(r){let e=0;for(let n=0;n<r.layers.length;n++)e+=r.layers[n].countParams();return e}async function re(r){for(let e=0;e<r.layers.length;e++)if(r.layersByDepth[e][0].dataFormat)return r.layersByDepth[e][0].dataFormat==="channelsLast"}async function Ee(r){return await ke(r)}async function ge(r){const e=r.max(),n=r.min();return await r.sub(n).div(e.sub(n))}function eo(r,e,n){const c=1,p=0,i=1,u=r.shape[4],a=Math.ceil(u/n);let f=null;for(let d=0;d<a;d++){const o=d*n,l=Math.min((d+1)*n,u)-o,t=ee(()=>r.slice([0,0,0,0,o],[-1,-1,-1,-1,l])),h=ee(()=>e.slice([0,0,0,o,0],[-1,-1,-1,l,-1])),g=Se(t,h,c,p,"NDHWC",i);t.dispose(),h.dispose();const y=je(g);if(g.dispose(),f===null)f=y;else{const m=f.add(y);f.dispose(),f!==y&&y.dispose(),f=m}ee(()=>{Me(ue([1,1]),ue([1,1]))})}return f}async function ye(r,e=.05,n=.95){const{qmin:c,qmax:p}=await Je(r,e,n),i=be(c),u=be(p),a=r.sub(i).div(u.sub(i));return i.dispose(),u.dispose(),a}async function se(r,e=1,n=1,c=1){if(r.rank!==3)throw new Error("Tensor must be 3D");const[p,i,u]=r.shape;return r.slice([e,n,c],[p-2*e,i-2*n,u-2*c])}async function te(r,e,n,c,p,i){const u=p[0],a=p[1],f=p[2],d=u+i[0]-1,o=a+i[1]-1,s=f+i[2]-1,l=n-d-1>0?n-d-1:0,t=c-o-1>0?c-o-1:0,h=e-s-1>0?e-s-1:0;return r.pad([[u,l],[a,t],[f,h]])}class oo{constructor(e,n,c,p,i=!0){this.model=e,this.outChannels=e.outputLayers[0].kernel.shape[4],this.chunkSize=n,this.isChannelLast=c,this.callbackUI=p,this.isWebWorker=i}async apply(e){const n=ie.get("WEBGL_DELETE_TEXTURE_THRESHOLD");ie.set("WEBGL_DELETE_TEXTURE_THRESHOLD",0);const c=this,p=performance.now(),i=c.model.layers[c.model.layers.length-1],u=i.getWeights()[0],a=i.getWeights()[1],f=c.isChannelLast?e.shape.slice(1,-1):e.shape.slice(2);let d=Re(ce(f),-1e4),o=ue(f),s=0;for(console.log(" channel loop");;){L().startScope();const l=await ee(()=>{const h=u.slice([0,0,0,0,s],[-1,-1,-1,-1,1]),g=a.slice([s],[1]),y=eo(e,h,Math.min(c.chunkSize,c.outChannels)).add(g),m=qe(y,d),P=_e(m,y,d),M=_e(m,ze(o.shape,s),o);return G([d,o,h,g,y,m]),ee(()=>Me(ce([1,1]),ce([1,1]))),[M,P]});console.log("======================="),c.callbackUI(`Iteration ${s}`,s/c.outChannels),c.isWebWorker||await new Promise(h=>setTimeout(h,17));const t=await H();if(console.log(`Number of Tensors: ${t.numTensors}`),console.log(`Number of Data Buffers: ${t.numDataBuffers}`),console.log(`Megabytes In Use: ${(t.numBytes/1048576).toFixed(3)} MB`),t.unreliable&&console.log(`Unreliable: ${t.unreliable}`),typeof o!="undefined"&&o.dispose(),typeof d!="undefined"&&d.dispose(),o=ve(l[0]),d=ve(l[1]),L().endScope(),s===c.outChannels-1){G(d);const g=performance.now()-p;return console.log(`Execution time for output layer: ${g} milliseconds`),ie.set("WEBGL_DELETE_TEXTURE_THRESHOLD",n),o}else{s++;const h=o.shape,g=o.dataSync(),y=o.shape,m=d.dataSync();o.dispose(),d.dispose(),o=ae(g,h),d=ae(m,y)}}}}async function Be(r,e,n,c,p,i,u,a,f,d,o,s){console.log(" ---- Start FullVolume Inference with Sequential Conv Layer for phase-II ---- "),e.enableQuantileNorm?(console.log("preModel Quantile normalization enabled"),c=await ye(c)):(console.log("preModel Min Max normalization enabled"),c=await ge(c));let t;if(a==null){const O=e.autoThreshold;O>0&&O<=1?t=await Le(c,O):(console.log("No valid crop threshold value"),t=await c.greater([0]).asType("bool"))}else t=await a.greater([0]).asType("bool");console.log(" mask_3d shape :  ",t.shape);const[h,g,y,m,P,M]=await Ae(t);t.dispose();const N=[h,y,P],X=[g-h+1,m-y+1,M-P+1],U=await c.slice([h,y,P],[g-h+1,m-y+1,M-P+1]);c.dispose();const x=e.cropPadding;let v=await Ce(U,[x,x],[x,x],[x,x]);if(console.log(" cropped slices_3d with padding shape:  ",v.shape),U.dispose(),r.drawBoundingVolume){let O=await se(v,x,x,x);return console.log(" outLabelVolume without padding shape :  ",O.shape),O=await te(O,p,i,u,N,X),console.log(" outLabelVolume final shape after resizing :  ",O.shape),Oe(de(O),r,e,d),O.dispose(),0}o.Brainchop_Ver="FullVolume";const w=await n;try{let O=performance.now();const j=performance.now();let C=0;const _=e.enableTranspose,J=e.inferenceDelay;console.log("Inference delay :",J),_?(v=await v.transpose(),console.log("Input transposed for pre-model")):console.log("Transpose not enabled for pre-model");let S=1;const Z=w.layers.length;console.log("res.layers.length ",Z);const b=re(w),Q=r.batchSize,W=r.numOfChan;let $;b?(w.layers[0].batchInputShape[1]=v.shape[0],w.layers[0].batchInputShape[2]=v.shape[1],w.layers[0].batchInputShape[3]=v.shape[2],$=[Q,w.layers[0].batchInputShape[1],w.layers[0].batchInputShape[2],w.layers[0].batchInputShape[3],W]):(w.layers[0].batchInputShape[2]=v.shape[0],w.layers[0].batchInputShape[3]=v.shape[1],w.layers[0].batchInputShape[4]=v.shape[2],$=[Q,W,w.layers[0].batchInputShape[2],w.layers[0].batchInputShape[3],w.layers[0].batchInputShape[4]]),console.log(" Model batch input shape : ",w.layers[0].batchInputShape),o.Input_Shape=JSON.stringify(w.layers[0].batchInputShape),o.Output_Shape=JSON.stringify(w.output.shape),o.Channel_Last=b,o.Model_Param=await me(w),o.Model_Layers=await he(w),o.Model=e.modelName,o.Seq_Conv=e.enableSeqConv,o.Extra_Info=null;const K=w.layers[w.layers.length-1];console.log("Output Layer : ",K);const E=b?K.outputShape[K.outputShape.length-1]:K.outputShape[1];console.log("Num of output channels : ",E);const I=[];I[0]=await v.reshape($);const R=window.setInterval(async function(){try{w.layers[S].activation.getClassName()!=="linear"?I[S]=await w.layers[S].apply(I[S-1]):I[S]=await $e(I[S-1],w.layers[S].getWeights()[0],w.layers[S].getWeights()[1],w.layers[S].strides,w.layers[S].padding,w.layers[S].dilationRate,3),G(I[S-1])}catch(F){const B="Your graphics card (e.g. Intel) may not be compatible with WebGL. "+F.message;return f(B,-1,B),window.clearInterval(R),L().endScope(),L().disposeVariables(),o.Inference_t=1/0,o.Postprocess_t=1/0,o.Status="Fail",o.Error_Type=F.message,o.Extra_Err_Info="Failed while model layer "+S+" apply",f("",-1,"",o),0}if(console.log("layer output Tensor shape : ",I[S].shape),console.log("layer count params ",w.layers[S].countParams()),w.layers[S].dispose(),I[S-1].dispose(),f("Layer "+S.toString(),(S+1)/Z),H().unreliable){const F="unreliable reasons :"+H().reasons;f(F,NaN,F)}if(S===Z-2){window.clearInterval(R);const B=await(await new oo(w,10,b,f,!1)).apply(I[S]);if(f("seqConvLayer Done"),G(I[S]),console.log(" Output tensor",B),console.log(" Output tensor shape : ",B.shape),B.shape.length!==3){const A="Output tensor shape should be 3 dims but it is "+B.shape.length;f(A,-1,A)}const q=((performance.now()-O)/1e3).toFixed(4);console.log(" find array max ");const k=await B.max().dataSync()[0];C<k&&(C=k);const V=C+1;if(console.log("Predicted num of segmentation classes",V),o.Actual_Labels=V,o.Expect_Labels=E,o.NumLabels_Match=V===E,V!==E){const A="expected "+E+" labels, but the predicted are "+V;f(A,-1,A)}let T=B.reshape([v.shape[0],v.shape[1],v.shape[2]]);G(B),_&&(console.log("outLabelVolume transposed"),T=T.transpose()),T=await se(T,x,x,x),console.log(" outLabelVolume without padding shape :  ",T.shape),T=await te(T,p,i,u,N,X),console.log(" outLabelVolume final shape after resizing :  ",T.shape);const le=e.filterOutWithPreMask;if(a!=null&&r.isBrainCropMaskBased&&le){const A=await pe(a);T=await T.mul(A)}O=performance.now(),console.log("Generating correct output");let ne;try{const A=await new Uint32Array(T.dataSync()),Y=T.shape,Ve=T.dtype;ne=await Ie(A,Y,Ve,p,V,i,u,e,r,s),console.log(" Phase-2 num of tensors after generateOutputSlicesV2: ",H().numTensors),G(T),L().endScope(),L().disposeVariables()}catch(A){L().endScope(),L().disposeVariables(),console.log("Error while generating output: ",A);const Y="Failed while generating output due to limited browser memory available";return f(Y,-1,Y),o.Inference_t=q,o.Postprocess_t=1/0,o.Status="Fail",o.Error_Type=A.message,o.Extra_Err_Info="Failed while generating output",f("",-1,"",o),0}const z=((performance.now()-O)/1e3).toFixed(4);return console.log("Processing the whole brain volume in tfjs for multi-class output mask took : ",((performance.now()-j)/1e3).toFixed(4)+"  Seconds"),o.Inference_t=q,o.Postprocess_t=z,o.Status="OK",f("",-1,"",o),f("Segmentation finished",0),d(ne,r,e),0}else S++},J)}catch(O){if(f(O.message,-1,O.message),console.log('If webgl context is lost, try to restore webgl context by visit the link <a href="https://support.biodigital.com/hc/en-us/articles/218322977-How-to-turn-on-WebGL-in-my-browser">here</a>'),H().unreliable){const j="unreliable reasons :"+H().reasons;f(j,NaN,j)}}}async function Ne(r,e,n,c,p,i,u,a,f,d,o,s){let l=[];console.log(" ---- Start FullVolume inference phase-II ---- "),u.enableQuantileNorm?(console.log("preModel Quantile normalization enabled"),e=await ye(e)):(console.log("preModel Min Max normalization enabled"),e=await ge(e));let h;if(i==null){const _=u.autoThreshold;_>0&&_<=1?h=await Le(e,_):(console.log("No valid crop threshold value"),h=await e.greater([0]).asType("bool"))}else h=i.greater([0]).asType("bool");console.log(" mask_3d shape :  ",h.shape);const[g,y,m,P,M,N]=await Ae(h);h.dispose();const X=[g,m,M];console.log("refVoxel :",X);const U=[y-g+1,P-m+1,N-M+1];console.log("boundVolSizeArr :",U);const x=e.slice([g,m,M],[y-g+1,P-m+1,N-M+1]);e.dispose();const v=u.cropPadding;let w=await Ce(x,[v,v],[v,v],[v,v]);if(console.log(" cropped slices_3d with padding shape:  ",w.shape),x.dispose(),f.drawBoundingVolume){let _=await se(w,v,v,v);return console.log(" outLabelVolume without padding shape :  ",_.shape),_=await te(_,n,c,p,X,U),console.log(" outLabelVolume final shape after resizing :  ",_.shape),Oe(de(_),f,u,d),_.dispose(),0}a.Brainchop_Ver="FullVolume";let O=performance.now(),j=[];const C=await r;try{O=performance.now();const _=performance.now();let J=0;const S=u.enableTranspose,Z=u.inferenceDelay;console.log("Inference delay :",Z),S?(w=w.transpose(),console.log("Input transposed for pre-model")):console.log("Transpose not enabled for pre-model");let b=1;const Q=C.layers.length;console.log("res.layers.length ",Q);const W=await re(C),$=f.batchSize,K=f.numOfChan;W?(C.layers[0].batchInputShape[1]=w.shape[0],C.layers[0].batchInputShape[2]=w.shape[1],C.layers[0].batchInputShape[3]=w.shape[2],j=[$,C.layers[0].batchInputShape[1],C.layers[0].batchInputShape[2],C.layers[0].batchInputShape[3],K]):(C.layers[0].batchInputShape[2]=w.shape[0],C.layers[0].batchInputShape[3]=w.shape[1],C.layers[0].batchInputShape[4]=w.shape[2],j=[$,K,C.layers[0].batchInputShape[2],C.layers[0].batchInputShape[3],C.layers[0].batchInputShape[4]]),console.log(" Model batch input shape : ",C.layers[0].batchInputShape),a.Input_Shape=JSON.stringify(C.layers[0].batchInputShape),a.Output_Shape=JSON.stringify(C.output.shape),a.Channel_Last=W,a.Model_Param=await me(C),a.Model_Layers=await he(C),a.Model=u.modelName,a.Extra_Info=null;const E=[];E[0]=w.reshape(j);const I=window.setInterval(async function(){try{E[b]=C.layers[b].apply(E[b-1])}catch(R){return o(R.message,-1,R.message),window.clearInterval(I),L().endScope(),L().disposeVariables(),a.Inference_t=1/0,a.Postprocess_t=1/0,a.Status="Fail",a.Error_Type=R.message,a.Extra_Err_Info="Failed while model layer "+b+" apply",o("",-1,"",a),0}if(o("Layer "+b.toString(),(b+1)/Q),console.log("layer output Tensor shape : ",E[b].shape),console.log("layer count params ",C.layers[b].countParams()),C.layers[b].dispose(),E[b-1].dispose(),H().unreliable){const R="unreliable reasons :"+H().reasons;o(R,NaN,R)}if(b===Q-1){window.clearInterval(I);const R=W?-1:1;console.log(" find argmax "),console.log("last Tensor shape : ",E[b].shape);const F=W?E[b].shape[4]:E[b].shape[1];let B;try{const z=performance.now();console.log(" Try tf.argMax for fullVolume .."),B=Pe(E[b],R),console.log("tf.argMax for fullVolume takes : ",((performance.now()-z)/1e3).toFixed(4))}catch(z){if(R===-1)try{const A=performance.now();console.log(" tf.argMax failed .. try argMaxLarge .."),window.alert("tensor2LightBuffer() is not dead code?"),window.alert("argMaxLarge() is not dead code?"),console.log("argMaxLarge for fullVolume takes : ",((performance.now()-A)/1e3).toFixed(4))}catch(A){const Y="argMax buffer couldn't be created due to limited memory resources.";return o(Y,-1,Y),window.clearInterval(I),L().endScope(),L().disposeVariables(),a.Inference_t=1/0,a.Postprocess_t=1/0,a.Status="Fail",a.Error_Type=A.message,a.Extra_Err_Info="prediction_argmax from argMaxLarge failed",o("",-1,"",a),0}else{const A="argMax buffer couldn't be created due to limited memory resources.";return o(A,-1,A),B.dispose(),window.clearInterval(I),L().endScope(),L().disposeVariables(),a.Inference_t=1/0,a.Postprocess_t=1/0,a.Status="Fail",a.Error_Type=z.message,a.Extra_Err_Info="prediction_argmax from argMaxLarge not support yet channel first",o("",-1,"",a),0}}console.log(" prediction_argmax shape : ",B.shape);const q=((performance.now()-O)/1e3).toFixed(4);G(E[b]);const k=await B.max().dataSync()[0];J<k&&(J=k);const V=J+1;if(console.log("numSegClasses",V),a.Actual_Labels=V,a.Expect_Labels=F,a.NumLabels_Match=V===F,V!==F){const z="expected "+F+" labels, but the predicted are "+V;o(z,-1,z)}let T=B.reshape([w.shape[0],w.shape[1],w.shape[2]]);G(B),S&&(console.log("outLabelVolume transposed"),T=T.transpose()),T=await se(T,v,v,v),console.log(" outLabelVolume without padding shape :  ",T.shape),T=await te(T,n,c,p,X,U),console.log(" outLabelVolume final shape after resizing :  ",T.shape);const le=u.filterOutWithPreMask;if(i!=null&&f.isBrainCropMaskBased&&le){const z=pe(i);T=T.mul(z)}O=performance.now(),console.log("Generating correct output");try{const z=new Uint32Array(T.dataSync()),A=T.shape,Y=T.dtype;G(T),L().endScope(),L().disposeVariables(),l=await Ie(z,A,Y,n,V,c,p,u,f,s),console.log(" Phase-2 num of tensors after generateOutputSlicesV2: ",H().numTensors)}catch(z){L().endScope(),L().disposeVariables();const A="Failed while generating output due to limited browser memory available";return o(A,-1,A),a.Inference_t=q,a.Postprocess_t=1/0,a.Status="Fail",a.Error_Type=z.message,a.Extra_Err_Info="Failed while generating output",o("",-1,"",a),0}const ne=((performance.now()-O)/1e3).toFixed(4);return L().disposeVariables(),console.log("Processing the whole brain volume in tfjs for multi-class output mask took : ",((performance.now()-_)/1e3).toFixed(4)+"  Seconds"),a.Inference_t=q,a.Postprocess_t=ne,a.Status="OK",o("",-1,"",a),clearInterval(I),o("Segmentation finished",0),d(l,f,u),0}b++},Z)}catch(_){o(_.message,-1,_.message),console.log('If webgl context is lost, try to restore webgl context by visit the link <a href="https://support.biodigital.com/hc/en-us/articles/218322977-How-to-turn-on-WebGL-in-my-browser">here</a>')}}async function no(r,e,n,c,p,i,u,a,f,d,o,s){if(a.No_SubVolumes=1,u.preModelId){const l=await Ee(f.rootURL+D[u.preModelId-1].path),t=D[u.preModelId-1].enableTranspose,h=D[u.preModelId-1].enableQuantileNorm;let g=null;h?(console.log("preModel Quantile normalization enabled"),g=await ye(e)):(console.log("preModel Min Max normalization enabled"),g=await ge(e)),t?(g=await g.transpose(),console.log("Input transposed for pre-model")):console.log("Transpose not enabled for pre-model"),a.Brainchop_Ver="PreModel_FV";const y=await l;try{const m=performance.now(),P=y,M=P.layers[0].batchInputShape;if(console.log(" Pre-Model batch input shape : ",M),M.length!==5){const b="The pre-model input shape must be 5D ";return o(b,-1,b),0}const N=re(P),X=f.batchSize,U=f.numOfChan;let x,v,w,O;if(N){if(console.log("Pre-Model Channel Last"),isNaN(M[4])||M[4]!==1){const b="The number of channels for pre-model input shape must be 1";return o(b,-1,b),0}x=M[1],v=M[2],w=M[3],O=[X,x,v,w,U]}else{if(console.log("Pre-Model Channel First"),isNaN(M[1])||M[1]!==1){const b="The number of channels for pre-model input shape must be 1";return o(b,-1,b),0}x=M[2],v=M[3],w=M[4],O=[X,U,x,v,w]}a.Input_Shape=JSON.stringify(O),a.Output_Shape=JSON.stringify(P.output.shape),a.Channel_Last=N,a.Model_Param=await me(P),a.Model_Layers=await he(P);let j=0;const C=D[u.preModelId-1].inferenceDelay;let _=1;const J=y.layers.length,S=[];S[0]=g.reshape(O),G(g);const Z=window.setInterval(async function(){try{S[_]=await y.layers[_].apply(S[_-1])}catch(b){const Q="Your graphics card (e.g. Intel) may not be compatible with WebGL. "+b.message;return o(Q,-1,Q),window.clearInterval(Z),L().endScope(),L().disposeVariables(),a.Inference_t=1/0,a.Postprocess_t=1/0,a.Status="Fail",a.Error_Type=b.message,a.Extra_Err_Info="PreModel Failed while model layer "+_+" apply",o("",-1,"",a),0}if(y.layers[_].dispose(),S[_-1].dispose(),o("Layer "+_.toString(),(_+1)/J),H().unreliable){const b="unreliable reasons :"+H().reasons;o(b,NaN,b)}if(_===J-1){window.clearInterval(Z);const b=N?-1:1;console.log(" find argmax "),console.log("last Tensor shape : ",S[_].shape);const Q=N?S[_].shape[4]:S[_].shape[1];let W;try{console.log(" Try tf.argMax for fullVolume .."),W=await Pe(S[_],b)}catch(q){if(b===-1)try{const k=performance.now();console.log(" tf.argMax failed .. try argMaxLarge .."),window.alert("tensor2LightBuffer() is not dead code?"),window.alert("argMaxLarge() is not dead code?"),console.log("argMaxLarge for fullVolume takes : ",((performance.now()-k)/1e3).toFixed(4))}catch(k){const V="argMax buffer couldn't be created due to limited memory resources.";return o(V,-1,V),W.dispose(),window.clearInterval(Z),L().endScope(),L().disposeVariables(),a.Inference_t=1/0,a.Postprocess_t=1/0,a.Status="Fail",a.Error_Type=k.message,a.Extra_Err_Info="preModel prediction_argmax from argMaxLarge failed",o("",-1,"",a),0}else{const k="argMax buffer couldn't be created due to limited memory resources.";return o(k,-1,k),W.dispose(),window.clearInterval(Z),L().endScope(),L().disposeVariables(),a.Inference_t=1/0,a.Postprocess_t=1/0,a.Status="Fail",a.Error_Type=q.message,a.Extra_Err_Info="preModel prediction_argmax from argMaxLarge not support yet channel first",o("",-1,"",a),0}}console.log(" Pre-model prediction_argmax shape : ",W.shape);const $=((performance.now()-m)/1e3).toFixed(4);G(S[_]),console.log(" Pre-model find array max ");const K=await W.max().dataSync()[0];j<K&&(j=K);const E=j+1;console.log("Pre-model numSegClasses",E),a.Actual_Labels=E,a.Expect_Labels=Q,a.NumLabels_Match=E===Q;let I=await W.reshape([n,c,p]);G(W),t&&(console.log("Pre-model outLabelVolume transposed"),I=I.transpose());const R=performance.now();console.log("Generating pre-model output");let F;try{const q=await de(I);F=await Ye(q,n,c,p,u,f,o,d,!1),await G(I),console.log(" Phase-1 num of tensors after generateBrainMask: ",H().numTensors)}catch(q){L().endScope(),L().disposeVariables();const k="Failed while generating pre-model output due to limited browser memory available";return o(k,-1,k),a.Inference_t=$,a.Postprocess_t=1/0,a.Status="Fail",a.Error_Type=q.message,a.Extra_Err_Info="Pre-model failed while generating output",o("",-1,"",a),0}const B=((performance.now()-R)/1e3).toFixed(4);if(console.log("Pre-model processing the whole brain volume in tfjs tooks for multi-class output mask : ",((performance.now()-m)/1e3).toFixed(4)+"  Seconds"),a.Inference_t=$,a.Postprocess_t=B,a.Status="OK",o("",-1,"",a),F==null){const q="slice_3d_mask failed ...";return o(q,-1,q),0}else if(console.log("--- pre-model done ---"),i){if(u.enableSeqConv)return console.log("------ Mask Cropping & Seq Convoluton ------"),await Be(f,u,r,e,n,c,p,F,o,d,a,s),0;console.log("------ Mask Cropping  -  NO Seq Convoluton ------"),await Ne(r,e,n,c,p,F,u,a,f,d,o,s)}else window.alert("inferenceSubVolumes() is not dead code?")}_++},C)}catch(m){o(m.message,-1,m.message),console.log('If webgl context is lost, try to restore webgl context by visit the link <a href="https://support.biodigital.com/hc/en-us/articles/218322977-How-to-turn-on-WebGL-in-my-browser">here</a>')}}else console.log("--- No pre-model is selected ---"),console.log("------ Run voxel cropping ------"),i?u.enableSeqConv?(console.log("------ Seq Convoluton ------"),await Be(f,u,r,e,n,c,p,null,o,d,a,s)):Ne(r,e,n,c,p,null,u,a,f,d,o,s):window.alert("inferenceSubVolumes() is not dead code?")}async function ao(r=!0){await Qe(),oe().set("DEBUG",!1),oe().set("WEBGL_FORCE_F16_TEXTURES",r),oe().set("WEBGL_DELETE_TEXTURE_THRESHOLD",0),await He(),console.log("tf env() flags :",oe().flags),console.log("tf env() features :",oe().features),console.log("tf env total features: ",Object.keys(oe().features).length),console.log(Te())}async function ro(r,e,n,c,p,i){const u=[];u.startTime=Date.now(),i("Segmentation started",0),performance.now();const a=r.batchSize,f=r.numOfChan;if(isNaN(a)||a!==1){const x="The batch Size for input shape must be 1";return i(x,-1,x),0}if(isNaN(f)||f!==1){const x="The number of channels for input shape must be 1";return i(x,-1,x),0}L().startScope(),console.log("Batch size: ",a),console.log("Num of Channels: ",f);const d=await Ee(r.rootURL+e.path);await ao(!0),u.TF_Backend=Te();const o=d;let s=[];if(s=o.layers[0].batchInputShape,console.log(" Model batch input shape : ",s),s.length!==5){const x="The model input shape must be 5D";return i(x,-1,x),0}let l,t,h;const g=n.dims[1],y=n.dims[2],m=n.dims[3];if(await re(o)){if(console.log("Model Channel Last"),isNaN(s[4])||s[4]!==1){const x="The number of channels for input shape must be 1";return i(x,-1,x),0}l=s[1],t=s[2],h=s[3]}else{if(console.log("Model Channel First"),isNaN(s[1])||s[1]!==1){const x="The number of channels for input shape must be 1";return i(x,-1,x),0}l=s[2],t=s[3],h=s[4]}let M;l===256&&t===256&&h===256?M=!0:M=!1,u.isModelFullVol=M;let N=await De(m,n,c);const X=e.enableTranspose,U=e.enableCrop;M&&(U?await no(d,N,m,y,g,M,e,u,r,p,i,c):(console.log("Cropping Disabled"),X?(N=N.transpose(),console.log("Input transposed")):console.log("Transpose NOT Enabled"),e.enableSeqConv?(console.log("Seq Convoluton Enabled"),window.alert("inferenceFullVolumeSeqCovLayer() is not dead code?")):(console.log("Seq Convoluton Disabled"),window.alert("inferenceFullVolume() is not dead code?"))))}async function we(){return navigator.userAgent.indexOf("OPR/")>-1?"Opera":navigator.userAgent.indexOf("Edg/")>-1?"Edge":navigator.userAgent.indexOf("Falkon/")>-1?"Falkon":navigator.userAgent.indexOf("Chrome/")>-1?"Chrome":navigator.userAgent.indexOf("Firefox/")>-1?"Firefox":navigator.userAgent.indexOf("Safari/")>-1?"Safari":navigator.userAgent.indexOf("MSIE/")>-1||navigator.userAgent.indexOf("rv:")>-1?"IExplorer":"Unknown"}async function so(){return navigator.userAgent.indexOf("OPR/")>-1?parseInt(navigator.userAgent.split("OPR/")[1]):navigator.userAgent.indexOf("Edg/")>-1?parseInt(navigator.userAgent.split("Edg/")[1]):navigator.userAgent.indexOf("Falkon/")>-1?parseInt(navigator.userAgent.split("Falkon/")[1]):navigator.userAgent.indexOf("Chrome/")>-1?parseInt(navigator.userAgent.split("Chrome/")[1]):navigator.userAgent.indexOf("Firefox/")>-1?parseInt(navigator.userAgent.split("Firefox/")[1]):navigator.userAgent.indexOf("Safari/")>-1?parseInt(navigator.userAgent.split("Safari/")[1]):navigator.userAgent.indexOf("MSIE/")>-1||navigator.userAgent.indexOf("rv:")>-1?parseInt(navigator.userAgent.split("MSIE/")[1]):1/0}async function to(){return navigator.userAgent.indexOf("Win")>-1?"Windows":navigator.userAgent.indexOf("Mac")>-1?"MacOS":navigator.userAgent.indexOf("Linux")>-1?"Linux":navigator.userAgent.indexOf("UNIX")>-1?"UNIX":"Unknown"}async function lo(r){return r?(console.log("WebGl2 is enabled"),!0):(console.log(typeof WebGL2RenderingContext!="undefined"?"WebGL2 may be disabled. Please try updating video card drivers":"WebGL2 is not supported"),!1)}async function io(r){let e;if(r&&(e=r.getExtension("WEBGL_debug_renderer_info"),e)){const n=r.getParameter(e.UNMASKED_VENDOR_WEBGL);return n.indexOf("(")>-1&&n.indexOf(")")>-1?n.substring(n.indexOf("(")+1,n.indexOf(")")):n}return null}async function co(r){if(r){const e=r.getExtension("WEBGL_debug_renderer_info");return e?r.getParameter(e.UNMASKED_VENDOR_WEBGL):null}else return null}async function uo(r){if(r){if(we()==="Firefox")return r.getParameter(r.RENDERER);const e=r.getExtension("WEBGL_debug_renderer_info");return e?r.getParameter(e.UNMASKED_RENDERER_WEBGL):null}else return null}async function po(r){let e;if(r){if(we()==="Firefox")return r.getParameter(r.RENDERER);if(e=r.getExtension("WEBGL_debug_renderer_info"),e){let n=r.getParameter(e.UNMASKED_RENDERER_WEBGL);return n.indexOf("(")>-1&&n.indexOf(")")>-1&&n.indexOf("(R)")===-1&&(n=n.substring(n.indexOf("(")+1,n.indexOf(")")),n.split(",").length===3)?n.split(",")[1].trim():n}}return null}async function fo(){return navigator.hardwareConcurrency}async function Fe(){return/Chrome/.test(navigator.userAgent)&&/Google Inc/.test(navigator.vendor)}async function ho(r,e=null){const n=new Date;if(r.isModelFullVol?r.Brainchop_Ver="FullVolume":r.Brainchop_Ver="SubVolumes",r.Total_t=(Date.now()-r.startTime)/1e3,delete r.startTime,r.Date=parseInt(n.getMonth()+1)+"/"+n.getDate()+"/"+n.getFullYear(),r.Browser=await we(),r.Browser_Ver=await so(),r.OS=await to(),r.WebGL2=await lo(e),r.GPU_Vendor=await io(e),r.GPU_Card=await po(e),r.GPU_Vendor_Full=await co(e),r.GPU_Card_Full=await uo(e),r.CPU_Cores=await fo(),r.Which_Brainchop="latest",await Fe()&&(r.Heap_Size_MB=window.performance.memory.totalJSHeapSize/(1024*1024).toFixed(2),r.Used_Heap_MB=window.performance.memory.usedJSHeapSize/(1024*1024).toFixed(2),r.Heap_Limit_MB=window.performance.memory.jsHeapSizeLimit/(1024*1024).toFixed(2)),e){console.log("MAX_TEXTURE_SIZE :",e.getParameter(e.MAX_TEXTURE_SIZE)),console.log("MAX_RENDERBUFFER_SIZE :",e.getParameter(e.MAX_RENDERBUFFER_SIZE));const c=e.getExtension("WEBGL_debug_renderer_info");console.log("VENDOR WEBGL:",e.getParameter(c.UNMASKED_VENDOR_WEBGL)),r.Texture_Size=e.getParameter(e.MAX_TEXTURE_SIZE)}else r.Texture_Size=null;return r}function mo(){return new Worker("/brainchop/assets/brainchop-webworker.38852e93.js",{type:"module"})}async function go(){dragMode.onchange=async function(){s.opts.dragMode=this.selectedIndex},drawDrop.onchange=async function(){if(s.volumes.length<2){window.alert("No segmentation open (use the Segmentation pull down)"),drawDrop.selectedIndex=-1;return}if(!s.drawBitmap){window.alert("No drawing (hint: use the Draw pull down to select a pen)"),drawDrop.selectedIndex=-1;return}const l=parseInt(this.value);if(l===0){s.drawUndo(),drawDrop.selectedIndex=-1;return}let t=s.volumes[1].img,h=await s.saveImage({filename:"",isSaveDrawing:!0});const g=352,y=h.length;if(l===1)for(let m=0;m<y;m++)h[g+m]>0&&(t[m]=1);if(l===2)for(let m=0;m<y;m++)h[g+m]>0&&(t[m]=0);s.closeDrawing(),s.updateGLVolume(),s.setDrawingEnabled(!1),penDrop.selectedIndex=-1,drawDrop.selectedIndex=-1},penDrop.onchange=async function(){const l=parseInt(this.value);s.setDrawingEnabled(l>=0),l>=0&&s.setPenValue(l&7,l>7)},aboutBtn.onclick=function(){window.alert("Drag and drop NIfTI images. Use pulldown menu to choose brainchop model")},diagnosticsBtn.onclick=function(){if(d.length<1){window.alert("No diagnostic string generated: run a model to create diagnostics");return}navigator.clipboard.writeText(d),window.alert(`Diagnostics copied to clipboard
`+d)},opacitySlider0.oninput=function(){s.setOpacity(0,opacitySlider0.value/255),s.updateGLVolume()},opacitySlider1.oninput=function(){s.setOpacity(1,opacitySlider1.value/255)};async function r(){const l=s.volumes[0];let t=l.dims[1]===256&&l.dims[2]===256&&l.dims[3]===256;if((l.permRAS[0]!==-1||l.permRAS[1]!==3||l.permRAS[2]!==-2)&&(t=!1),t)return;const h=await s.conform(l,!1);await s.removeVolume(s.volumes[0]),await s.addVolume(h)}async function e(){for(;s.volumes.length>1;)await s.removeVolume(s.volumes[1])}modelSelect.onchange=async function(){this.selectedIndex<0&&(modelSelect.selectedIndex=11),await e(),await r();const l=D[this.selectedIndex],t=Ze;if(t.rootURL=location.href,Boolean(window.location.hostname==="localhost"||window.location.hostname==="[::1]"||window.location.hostname.match(/^127(?:\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}$/))&&(t.rootURL=location.protocol+"//"+location.host),workerCheck.checked){if(typeof o!="undefined"){console.log("Unable to start new segmentation: previous call has not completed");return}o=await new mo;const g={datatypeCode:s.volumes[0].hdr.datatypeCode,dims:s.volumes[0].hdr.dims},y={opts:t,modelEntry:l,niftiHeader:g,niftiImage:s.volumes[0].img};o.postMessage(y),o.onmessage=function(m){const P=m.data.cmd;P==="ui"&&(m.data.modalMessage!==""&&(o.terminate(),o=void 0),u(m.data.message,m.data.progressFrac,m.data.modalMessage,m.data.statData)),P==="img"&&(o.terminate(),o=void 0,p(m.data.img,m.data.opts,m.data.modelEntry))}}else ro(t,l,s.volumes[0].hdr,s.volumes[0].img,p,u)},saveImgBtn.onclick=function(){s.volumes[1].saveToDisk("Custom.nii")},saveSceneBtn.onclick=function(){s.saveDocument("brainchop.nvd")},workerCheck.onchange=function(){modelSelect.onchange()},clipCheck.onchange=function(){clipCheck.checked?s.setClipPlane([0,0,90]):s.setClipPlane([2,0,90])};function n(){opacitySlider0.oninput()}async function c(l){return await(await fetch(l)).json()}async function p(l,t,h){e();const g=await s.volumes[0].clone();if(g.zeroImage(),g.hdr.scl_inter=0,g.hdr.scl_slope=1,g.img=new Uint8Array(l),h.colormapPath){const y=await c(h.colormapPath);g.setColormapLabel(y),g.hdr.intent_code=1002}else{let y=t.atlasSelectedColorTable.toLowerCase();s.colormaps().includes(y)||(y="actc"),g.colormap=y}g.opacity=opacitySlider1.value/255,await s.addVolume(g)}async function i(l){(typeof l=="string"||l instanceof String)&&(l=function(h){const g=JSON.parse(h),y=[];for(const m in g)y[m]=g[m];return y}(l)),l=await ho(l,s.gl),d=`:: Diagnostics can help resolve issues https://github.com/neuroneural/brainchop/issues ::
`;for(const t in l)d+=t+": "+l[t]+`
`}function u(l="",t=-1,h="",g=[]){l!==""&&(console.log(l),document.getElementById("location").innerHTML=l),isNaN(t)?(memstatus.style.color="red",memstatus.innerHTML="Memory Issue"):t>=0&&(modelProgress.value=t*modelProgress.max),h!==""&&window.alert(h),Object.keys(g).length>0&&i(g)}function a(l){document.getElementById("location").innerHTML="&nbsp;&nbsp;"+l.string}const f={backColor:[.4,.4,.4,1],show3Dcrosshair:!0,onLocationChange:a};let d="",o;const s=new Xe(f);s.attachToCanvas(gl1),s.opts.dragMode=s.dragModes.pan,s.opts.multiplanarForceRender=!0,s.opts.yoke3Dto2DZoom=!0,s.opts.crosshairGap=11,s.setInterpolation(!0),await s.loadVolumes([{url:"./t1_crop.nii.gz"}]);for(let l=0;l<D.length;l++){const t=document.createElement("option");t.text=D[l].modelName,t.value=D[l].id.toString(),modelSelect.appendChild(t)}s.onImageLoaded=n,modelSelect.selectedIndex=-1,drawDrop.selectedIndex=-1,workerCheck.checked=await Fe()}go();
